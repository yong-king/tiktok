# 短视频项目

# 环境搭建

## mysql：

```bash
docker pull mysql:8.0.36

docker run --name mysql -v /d/my_program/docker/mysql/data:/var/lib/mysql -v /d/my_program/docker/mysql/conf:/etc/mysql/conf.d -v /d/my_program/docker/mysql/log:/var/log/mysql -e MYSQL_ROOT_PASSWORD=root -p 3306:3306 -d mysql:8.0.36

```

user：

```sql
CREATE DATABASE tiktok DEFAULT CHARACTER SET utf8mb4 COLLATE utf8mb4_general_ci;

CREATE TABLE `users` (
                         `id` BIGINT UNSIGNED NOT NULL PRIMARY KEY COMMENT '用户ID，雪花算法生成',
                         `username` VARCHAR(64) NOT NULL UNIQUE COMMENT '用户名，唯一',
                         `password_hash` VARCHAR(255) NOT NULL COMMENT '加密后的密码',
                         `avatar` VARCHAR(255) DEFAULT NULL COMMENT '用户头像URL',
                         `background_image` VARCHAR(255) DEFAULT NULL COMMENT '个人页背景图',
                         `signature` VARCHAR(255) DEFAULT NULL COMMENT '个性签名',
                         `follow_count` INT UNSIGNED NOT NULL DEFAULT 0 COMMENT '关注数',
                         `follower_count` INT UNSIGNED NOT NULL DEFAULT 0 COMMENT '粉丝数',
                         `work_count` INT UNSIGNED NOT NULL DEFAULT 0 COMMENT '作品数',
                         `favorite_count` INT UNSIGNED NOT NULL DEFAULT 0 COMMENT '喜欢数',
                         `total_favorited` INT UNSIGNED NOT NULL DEFAULT 0 COMMENT '获赞总数',
                         `tags` JSON DEFAULT NULL COMMENT '标签（AI画像使用）',
                         `status` TINYINT NOT NULL DEFAULT 1 COMMENT '账号状态：1正常，0封禁',
                         `extra` JSON DEFAULT NULL COMMENT '扩展字段，预留给未来功能',
                         `reserved1` VARCHAR(255) DEFAULT NULL COMMENT '预留字段1',
                         `reserved2` VARCHAR(255) DEFAULT NULL COMMENT '预留字段2',
                         `created_at` TIMESTAMP DEFAULT CURRENT_TIMESTAMP COMMENT '创建时间',
                         `updated_at` TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT '更新时间',
                         `deleted_at` TIMESTAMP NULL DEFAULT NULL COMMENT '软删除时间'
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COMMENT='用户基础表';

```

### gorm

```protobuf
go get -u gorm.io/gorm
go get -u gorm.io/driver/sqlite
```

## redis：

```bash
docker pull redis:7.2.4
	
docker run --name redis -v /d/my_program/docker/redis:/data -p 6379:6379 -d redis:7.2.4
```

## kratos

安装

```go
go install github.com/go-kratos/kratos/cmd/kratos/v2@latest
```

## protoc

```go
// 下载protoc 
https://github.com/protocolbuffers/protobuf/releases

// bin/protoc.exe
// 复制到gopath
// protoc.exe 所在的目录路径加入到环境变量 %PATH% 中
// protoc --version
```

# uesr-service

```protobuf
syntax = "proto3";
option go_package = "user/api/user/v1;v1";
package user;

import "google/api/annotations.proto";

service UserService {
  rpc Register(RegisterRequest) returns (RegisterReply) {
    option (google.api.http) = {
      post: "/api/user/register"
      body: "*"
    };
  };
  rpc Login(LoginRequest) returns (LoginReply) {
    option (google.api.http) = {
      post: "/api/user/login"
      body: "*"
    };
  };
  rpc UserInfo(UserInfoRequest) returns (UserInfoReply) {
    option (google.api.http) = {
      get: "/api/user"
    };
  }
}

//  =========================用户注册============================

message RegisterRequest {
  string username = 1; // 用户名，最长为32位
  string password = 2; // 密码，最长位32位
}

message RegisterReply {
  int32 status_code = 1;
  string status_msg = 2;
  int64 user_id = 3;
  string token = 4;
}

//  ==========================用户登录============================
message LoginRequest {
  string username = 1;
  string password = 2;
}

message LoginReply {
  int32 status_code = 1;
  string status_msg = 2;
  int64 user_id = 3;
  string token = 4;
}

//  ===========================用户信息===========================
message UserInfoRequest {
  int64 user_id = 1;
  int64 current_user_id = 2; // 当前登录用户ID（用于判断是否关注）
}

message UserInfoReply {
  int32 status_code = 1;
  string status_msg = 2;
  User user = 3;
}

message User {
  int64 id = 1; // 用户id
  string name = 2;  // 用户名称
  int64 follow_count = 3; // 关注总数
  int64 follower_count = 4; // 粉丝总数
  bool is_follow = 5; // true-已关注，false-未关注
  string avatar = 6;  // 用户头像
  string background_image = 7;  // 用户个人页顶部大图
  string signature = 8; // 个人简介
  int64 total_favorited = 9;  // 获赞数量
  int64 work_count = 10;  // 作品数量
  int64 favorite_count = 11;  // 点赞数量
}
```

```protobuf
// 生成client
kratos proto client api/user/v1/user.proto

// 生成service
kratos proto server api/user/v1/user.proto -t internal/service
```

## 用户注册

```go
kratos new user-service
```

1. 检查用户是否存在
2. 用户密码加密
3. 雪花算法生成user_id，jwt生成token
4. 注册用户
5. 返回用户信息
6. 添加逻辑
    1. 用户校验，密码为空也返回，直接报错

## 用户登录

1. 用户身份校验
    1. 根据id和密码
2. jwt生成token，判断token是否过期
    1. 过期刷新
3. 返回响应
4. 扩展
    1. 退登录  ——→ state 0,    登录 ——→ 1， 用户异常状态 ————> 3（封号）
    2. 同时只能由一个在线
    3. 用户名或者密码为空直接返回报错登录失败

## 获取用户信息

1. 当前用户、被获取的用户的信息
2. 根据用户id获取用户信息
3. 扩展
    1. 当前用户与被查询用户的关系
    2. 用户名或者密码为空直接返回报错登录失败
        1. 这里可能需要调整，比如应该只是获取个人的信息
            1. 判断id是否为空
            2. 解析token 与当前id是否相同
            3. 根据id去查库

## 用户信息的添加

1. 用户背景
2. 用户头像
3. 用户标签
4. 用户标签
5. 性别
6. 年龄
7. 地域

# video-service

### 数据库sql

```sql
CREATE TABLE `videos` (
  `id` BIGINT UNSIGNED NOT NULL PRIMARY KEY COMMENT '视频ID（雪花ID）',
  `user_id` BIGINT UNSIGNED NOT NULL COMMENT '作者用户ID',

  `play_url` TEXT NOT NULL COMMENT '视频播放地址',
  `cover_url` TEXT NOT NULL COMMENT '视频封面地址',
  `title` VARCHAR(255) NOT NULL COMMENT '视频标题',
  `description` TEXT DEFAULT NULL COMMENT '视频描述',
  `duration` FLOAT DEFAULT 0 COMMENT '视频时长（秒）',
  `tags` TEXT DEFAULT NULL COMMENT '视频标签',

  `favorite_cnt` INT DEFAULT 0 COMMENT '点赞数',
  `comment_cnt` INT DEFAULT 0 COMMENT '评论数',
  `share_cnt` INT DEFAULT 0 COMMENT '分享数',
  `collect_cnt` INT DEFAULT 0 COMMENT '收藏数',

  `is_public` TINYINT(1) DEFAULT 1 COMMENT '是否公开（0私密，1公开）',
  `audit_status` TINYINT DEFAULT 1 COMMENT '审核状态（0审核中，1通过，2拒绝）',
  `is_original` TINYINT(1) DEFAULT 1 COMMENT '是否原创（1原创，0非原创）',
  `source_url` TEXT DEFAULT NULL COMMENT '原视频地址（非原创来源）',
  `transcode_status` TINYINT DEFAULT 1 COMMENT '转码状态（0失败，1成功，2转码中）',

  `video_width` INT DEFAULT NULL COMMENT '视频宽度（像素）',
  `video_height` INT DEFAULT NULL COMMENT '视频高度（像素）',

  `biz_ext` JSON DEFAULT NULL COMMENT '通用扩展字段（存储动态属性如推荐来源等）',
  `reserved_1` VARCHAR(255) DEFAULT NULL COMMENT '预留字段1',
  `reserved_2` VARCHAR(255) DEFAULT NULL COMMENT '预留字段2',

  `created_at` DATETIME DEFAULT CURRENT_TIMESTAMP COMMENT '创建时间（视频发布时间）',
  `update_time` DATETIME DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT '最后更新时间',
  `delete_at` DATETIME DEFAULT NULL COMMENT '软删除时间（非空表示被删除）',

  -- 单字段索引
  KEY `idx_user_id` (`user_id`) COMMENT '根据用户查视频',
  KEY `idx_created_at` (`created_at`) COMMENT '按时间倒序展示',

  -- 联合索引
  KEY `idx_user_time` (`user_id`, `created_at` DESC) COMMENT '用户视频列表分页用',
  KEY `idx_public_audit_time` (`is_public`, `audit_status`, `created_at` DESC) COMMENT '视频流按时间筛选已审核的公开视频',

  -- 排序类索引（热门、推荐排序使用）
  KEY `idx_favorite_cnt` (`favorite_cnt` DESC) COMMENT '根据点赞数排序',
  KEY `idx_comment_cnt` (`comment_cnt` DESC) COMMENT '根据评论数排序',
  KEY `idx_share_cnt` (`share_cnt` DESC) COMMENT '根据分享数排序',

  -- 审核后台筛选用
  KEY `idx_audit_status` (`audit_status`) COMMENT '后台审核快速筛选'
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COMMENT='视频信息表';

```

## minio

```sql
mkdir -p ./minio/data

docker run -d --name minio \
   -p 9000:9000 \
   -p 9001:9001 \
   -e "MINIO_ROOT_USER=admin" \
   -e "MINIO_ROOT_PASSWORD=admin123" \
   -v ./minio/data:/data \
   quay.io/minio/minio:RELEASE.2023-12-20T01-00-02Z \
   server /data --console-address ":9001"

用户名：admin
密码：admin123
```

### 创建桶：video-flies,设置为public

```sql
package pkg

import (
	"context"
	"fmt"
	"github.com/minio/minio-go/v7"
	"github.com/minio/minio-go/v7/pkg/credentials"
	"io"
	"video-service/internal/conf"
)

type MinioUploader struct {
	client     *minio.Client // MinIO 客户端
	bucketName string        // 存储桶名称
	endpoint   string        // MinIO 访问地址（含端口）
}

// NewMinioUploader 初始化 MinIO 客户端并确保 bucket 存在
func NewMinioUploader(cfg *conf.Data_MinIO) (*MinioUploader, error) {
	// 创建 MinIO 客户端
	client, err := minio.New(cfg.Endpoint, &minio.Options{
		Creds:  credentials.NewStaticV4(cfg.AccessKeyID, cfg.SecretAccessKey, ""),
		Secure: cfg.UseSSL,
	})
	if err != nil {
		return nil, err
	}

	// 检查 bucket 是否存在，不存在则创建
	ctx := context.Background()
	exists, err := client.BucketExists(ctx, cfg.BucketName)
	if err != nil {
		return nil, err
	}

	if !exists {
		err = client.MakeBucket(ctx, cfg.BucketName, minio.MakeBucketOptions{})
		if err != nil {
			return nil, err
		}
	}

	return &MinioUploader{
		client:     client,
		bucketName: cfg.BucketName,
		endpoint:   cfg.Endpoint,
	}, nil
}

// Upload 上传文件到 MinIO 并返回外部可访问的 URL
func (u *MinioUploader) Upload(ctx context.Context, objectName string, reader io.Reader, size int64, contentType string) (string, error) {
	// 上传对象
	_, err := u.client.PutObject(ctx, u.bucketName, objectName, reader, size, minio.PutObjectOptions{ContentType: contentType})
	if err != nil {
		return "", err
	}
	// 构建播放地址（假设 MinIO 配置了公共访问）
	playURL := fmt.Sprintf("http://%s/%s/%s", u.endpoint, u.bucketName, objectName)
	return playURL, nil
}

```

## 上传视频

<aside>
💡

- 服务端拿到路径没法保证它在 **正确的机器、正确的文件系统** 有权限读这个路径（尤其是容器化、K8s 中每个 Pod 的本地文件系统是隔离的）。
- 如果客户端传的是路径，等于暴露了服务端的文件系统，带来很大的安全风险。
- 在多台机器、多副本部署时，本地路径不同，根本找不到同一个文件。
- **正确做法**：要么客户端把二进制直接传，要么前端直传到对象存储（S3、MinIO、OSS），然后把链接告诉后端。
- 👉 **gRPC** 里可以传小文件，或者传切片流式传输（要实现 `stream`），但大文件推荐用直传。
- 👉 所以最开始直接传路径，没意义且有安全风险。
- 
- gRPC 协议要求你在请求中直接放置文件的二进制数据，
    
    而 **“路径”只是字符串，不包含真实文件内容**，
    
    gRPC 服务端也没有权限访问客户端（或 Postman）的本地文件系统去读此路径。
    

> 做 if in.Data == nil { 读取路径读入 []byte }
> 
> 
> 这只能在**服务端**硬编码用本地文件做调试（即只能用于服务端本地开发单测），
> 
> **无法用于真实上传**，因为用户上传场景下服务器并不持有用户本地文件路径和权限。
> 
> ### **为什么不适合生产：**
> 
> - 用户上传视频时不会把文件路径发给你，而是上传真实文件二进制。
> - 路径上传只适合开发阶段做本地假数据调试，不可扩展到用户真实上传。
> - 它与标准的 HTTP 上传、前端直传 OSS/MinIO 的上传模式完全不兼容。
> - 以后你对接 iOS/Android/Web SDK 时都无法使用这种方式上传。

</aside>

<aside>
💡

`Gin` 用的是 **HTTP 协议**，它天然支持浏览器/客户端通过 `multipart/form-data` 直接上传文件（文件字段以 `Content-Type: multipart/form-data` 传）。

所以：

- 客户端传的是 **真实的文件内容**，不是路径，服务端通过 `c.FormFile("file")` 从 `POST` body 中拿到二进制文件流，这没安全隐患。
- Gin 的路由直接接收文件流，文件流只存在于请求里，**没有路径依赖**，跨容器、跨多副本都没问题。

所以用 Gin 做文件上传，跟浏览器表单上传一个原理，这就是为什么生产里大文件直传都会走 HTTP，上传完再告诉后端。

- Gin 能“直接上传文件”
- 符合用户真实上传场景
- 支持大文件分片、断点续传、前端直传等可扩展功能
</aside>

1. 初始化gin服务
    
    ```sql
    func NewGinServer() *gin.Engine {
    	r := gin.Default()
    
    	// 上传视频接口
    	r.POST("/api/video/upload", func(c *gin.Context) {
    		service.GlobalVideoService.UploadVideoGin(c)
    	})
    
    	return r
    }
    
    	// 启动 Gin server
    	go func() {
    		r := server.NewGinServer()
    		if err := r.Run(":8090"); err != nil {
    			panic(err)
    		}
    	}()
    ```
    
2. gin逻辑
    1. 绑定videoservice
        
        <aside>
        💡
        
        - 在 Kratos 架构下：
            - 所有 Usecase、Repo、MinioUploader、JWTManager 等依赖都通过 `wire` 构造到 `VideoService` 中。
            - 如果你在 Gin 里自己 new，就会失去配置、日志、上下文等集中管理。
            - 无法保证与 gRPC 侧调用行为一致（如统一 token 校验、上传逻辑、业务流）。
            - `VideoService` 既可以被 gRPC 调，也可以被 HTTP 调，完全一样的业务逻辑，只是入口不同。
            - 所以才需要：`BindVideoService` 让全局可用，Gin 只管拿到它调用。
            
            ✅ 保持
            
            **Gin 和 gRPC 调用的业务逻辑完全一致**
            
            （同一个
            
            ```
            VideoService
            ```
            
            ）
            
            ✅ 便于后续做接口网关、限流、监控、Tracing
            
            ✅ 保留 Kratos 项目的一致性
            
            ✅ 方便单元测试、集成测试
            
            ✅ 避免写两份重复上传逻辑
            
        </aside>
        
        ```sql
        var GlobalVideoService *VideoService
        
        func BindVideoService(svc *VideoService) {
        	GlobalVideoService = svc
        }
        
        func newAppWithService(
        	logger log.Logger,
        	gs *grpc.Server,
        	hs *http.Server,
        	videoService *service.VideoService,
        ) (*kratos.App, func(), error) {
        	// 绑定可供 Gin 使用的全局 VideoService
        	service.BindVideoService(videoService)
        
        	app := newApp(logger, gs, hs)
        	cleanup := func() {
        		log.NewHelper(logger).Info("cleanup called")
        	}
        	return app, cleanup, nil
        }
        
        // wireApp init kratos application.
        func wireApp(*conf.Server, *conf.Data, log.Logger, *conf.JWT, *conf.Data_MinIO) (*kratos.App, func(), error) {
        	panic(wire.Build(server.ProviderSet, data.ProviderSet, biz.ProviderSet, service.ProviderSet, pkg.ProviderSet, newAppWithService))
        }
        ```
        
    2. 获取数据
        
        ```sql
        file, err := c.FormFile("file")
        token := c.PostForm("token")
        refreshToken := c.PostForm("refreshToken")
        ```
        
    3. 读取文件
        
        ```sql
        f, err := file.Open()
        defer f.Close()
        data, err := io.ReadAll(f)
        ```
        
    4. grpc请求
        
        ```sql
        reply, err := s.UploadVideo(c, &v1.UploadVideoRequest{
        		Data:         data,
        		Filename:     file.Filename,
        		Token:        token,
        		RefreshToken: refreshToken,
        	})
        ```
        
3. grpc 处理
    1. 校验参数token，data，filename
    2. 上传
        
        ```sql
        reader := bytes.NewReader(in.Data)
        	playURL, err := s.uc.UploadVideo(ctx, objectName, reader, int64(len(in.Data)), "video/mp4")
        ```
        
    3. 生成视频地址

## 创建视频

1. 生成视频数据库gen

```sql
package main

// gorm gen configure

import (
	"fmt"

	"gorm.io/driver/mysql"
	"gorm.io/gorm"

	"gorm.io/gen"
)

const MySQLDSN = "root:root@tcp(127.0.0.1:3306)/tiktok?charset=utf8mb4&parseTime=True"

func connectDB(dsn string) *gorm.DB {
	db, err := gorm.Open(mysql.Open(dsn))
	if err != nil {
		panic(fmt.Errorf("connect db fail: %w", err))
	}
	return db
}

func main() {
	// 指定生成代码的具体相对目录(相对当前文件)，默认为：./query
	// 默认生成需要使用WithContext之后才可以查询的代码，但可以通过设置gen.WithoutContext禁用该模式
	g := gen.NewGenerator(gen.Config{
		// 默认会在 OutPath 目录生成CRUD代码，并且同目录下生成 model 包
		// 所以OutPath最终package不能设置为model，在有数据库表同步的情况下会产生冲突
		// 若一定要使用可以通过ModelPkgPath单独指定model package的名称
		OutPath: "./internal/data/query",
		/* ModelPkgPath: "dal/model"*/

		// gen.WithoutContext：禁用WithContext模式
		// gen.WithDefaultQuery：生成一个全局Query对象Q
		// gen.WithQueryInterface：生成Query接口
		Mode: gen.WithDefaultQuery | gen.WithQueryInterface,
	})

	// 通常复用项目中已有的SQL连接配置db(*gorm.DB)
	// 非必需，但如果需要复用连接时的gorm.Config或需要连接数据库同步表信息则必须设置
	g.UseDB(connectDB(MySQLDSN))

	// 从连接的数据库为所有表生成Model结构体和CRUD代码
	// 也可以手动指定需要生成代码的数据表
	g.ApplyBasic(g.GenerateAllTable()...)

	// 执行并生成代码
	g.Execute()
}

```

1. 逻辑
    1. 校验参数（token、title）
    2. 上传视频
    3. 视频title和description、tag的敏感词过滤，长度限制 TODO
    4. 生成视频的video_id（雪花算法）

## 获取用户视频列表

1. 参数校验
    1. 获取token中的查阅用户id
    2. 查找查阅用户id是否合法
    3. 查找被查阅用户id是否合法
        1. rpc调用（consul）user-service注册
        
        ```sql
        // proto 容器中的consul能检测到
        server:
          http:
            addr: 0.0.0.0:8080
            timeout: 1s
          grpc:
            addr: 0.0.0.0:9090
            timeout: 1s 
        registry:
          consul:
            addr: 127.0.0.1:8500
        service:
          name: user-service
          version: v1.0.0
        
        // cmd
        
        func newApp(logger log.Logger, gs *grpc.Server, hs *http.Server, consulAddr string) *kratos.App {
        	// new consul client
        	**consulCfg := api.DefaultConfig()
        	consulCfg.Address = consulAddr
        	client, err := api.NewClient(consulCfg)
        	if err != nil {
        		panic(err)
        	}
        	reg := consul.New(client)**
        
        	return kratos.New(
        		kratos.ID(id),
        		kratos.Name(Name),
        		kratos.Version(Version),
        		kratos.Metadata(map[string]string{}),
        		kratos.Logger(logger),
        		kratos.Server(
        			gs,
        			hs,
        		),
        		kratos.Registrar(reg),
        	)
        }
        
        	consulAddr := bc.Registry.Consul.Addr
        	Name = bc.Service.Name
        	Version = bc.Service.Version
        	// 这里有个踩坑
        	id = fmt.Sprintf("%s-%s", Name, bc.Server.Http.Addr)
        	// 不然consul一直有问题
        	
        	func NewData(c *conf.Data, logger log.Logger, jwt *pkg.JWTManager, upload *pkg.MinioUploader, db *gorm.DB, rdb *redis.Client, idg *pkg.IDGenerator, rr *consul.Registry) (*Data, func(), error) {
        	cleanup := func() {
        		log.NewHelper(logger).Info("closing the data resources")
        	}
        	query.SetDefault(db)
        
        	conn, err := grpc.DialInsecure(
        		context.Background(),
        		grpc.WithEndpoint(c.UserService.Endpoint),
        		grpc.WithDiscovery(rr),
        	)
        	if err != nil {
        		return nil, nil, err
        	}
        	return &Data{log: log.NewHelper(logger), jwt: jwt, uploade: upload, db: db, rdb: rdb, idg: idg, query: query.Q, UserClient: pbUser.NewUserServiceClient(conn)}, cleanup, nil
        }
        ```
        
    4. video-service
        
        ```go
        data: 
          user_service:
            endpoint: discovery:///user-service
        registry:
          consul:
            addr: 127.0.0.1:8500
        service:
          name: user-service
          version: v1.0.0
          
        // main
        	consulCfg := api.DefaultConfig()
        	consulCfg.Address = bc.Registry.Consul.Addr
        	client, err := api.NewClient(consulCfg)
        	if err != nil {
        		panic(err)
        	}
        	reg := consul.New(client)
        	
        	app, cleanup, err := wireApp(bc.Server, bc.Data, logger, bc.Jwt, bc.Data.Minio, bc.IdGen, reg)
        	
        	func newApp(logger log.Logger, gs *grpc.Server, hs *http.Server, reg *consul.Registry) *kratos.App {
        	return kratos.New(
        		kratos.ID(id),
        		kratos.Name(Name),
        		kratos.Version(Version),
        		kratos.Metadata(map[string]string{}),
        		kratos.Logger(logger),
        		kratos.Server(
        			gs,
        			hs,
        		),
        		kratos.Registrar(reg),
        	)
        }
        
        func wireApp(*conf.Server, *conf.Data, log.Logger, *conf.JWT, *conf.Data_MinIO, *conf.IDGen, *consul.Registry) (*kratos.App, func(), error) {
        	panic(wire.Build(server.ProviderSet, data.ProviderSet, biz.ProviderSet, service.ProviderSet, pkg.ProviderSet, newAppWithService))
        }
        
        ```
        
2. 根据被查阅的用户id去查找视频列表
    1. 按照分页来返回

## 模糊获取视频

1. 根据title模糊获取视频
2. 从es中模糊获取
3. 从mysql中模糊获取（兜底）

# Feed-Service

获取视频流

1. 游客模式
2. 非游客模式（user  service 微服务调用）
    1. 解析token
    2. 验证token
3. 获取视频流(批量返回视频列表)
    1. 查询时间的起点
    2. 查询数量，再查询时间前多少个
    3. 数据库查询
    4. 后期：按时间和热度计算分数查询
4. 获取视频作者信息
    1. 根据视频流中作者id获取作者信息
    2. user  service 微服务调用，批量获取
5. 点赞信息
    1. 当前用户是否点赞
6. 用户关系信息
    1. 当前用户是否是作者的粉丝

# Favorite-service

```sql
CREATE TABLE IF NOT EXISTS `favorite` (
                                          `id` BIGINT UNSIGNED NOT NULL AUTO_INCREMENT COMMENT '主键ID',
                                          `user_id` BIGINT UNSIGNED NOT NULL COMMENT '用户ID',
                                          `video_id` BIGINT UNSIGNED NOT NULL COMMENT '视频ID',
                                          `created_at` TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '点赞时间',
                                          `updated_at` TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT '更新时间',
                                          PRIMARY KEY (`id`),
    UNIQUE KEY `idx_user_video` (`user_id`, `video_id`),
    INDEX `idx_video_id` (`video_id`)
    ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci COMMENT='用户点赞表';

```

## 点赞

1. 参数校验
    1. token
    2. 点赞类型
    3. 点赞视频的id
2. 视频点赞
    1. 类型为1点赞
        1. 判断之前是否有过操作，幂等
        2. 事务，点赞数据库，视频数据库点赞计数
    2. 类型为2取消点赞
        1. 事务，点赞数据库删除，视频数据库点赞计数-1

## 获取用户点赞列表

1. 参数校验
    1. 被查询用户的id
    2. 当前用户的id，token解析
2. 根据被查询用户id查询点赞的视频id
3. 根据点赞视频的id获取视频信息

<aside>
💡

// 这是consul中也要修改的
`id = fmt.Sprintf("%s-%s", Name, bc.Server.Http.Addr)`

</aside>

# comment-service

```sql
CREATE TABLE IF NOT EXISTS `comment` (
                                         `id` BIGINT UNSIGNED NOT NULL AUTO_INCREMENT COMMENT '主键ID',
                                         `user_id` BIGINT UNSIGNED NOT NULL COMMENT '用户ID',
                                         `video_id` BIGINT UNSIGNED NOT NULL COMMENT '视频ID',
                                         `parent_id` BIGINT UNSIGNED DEFAULT 0 COMMENT '父评论ID，0表示一级评论',
                                         `content` TEXT NOT NULL COMMENT '评论内容',
                                         `is_deleted` TINYINT(1) NOT NULL DEFAULT 0 COMMENT '是否删除：0-未删除，1-已删除',
    `created_at` TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '评论时间',
    `updated_at` TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT '更新时间',
    PRIMARY KEY (`id`),
    INDEX `idx_video_id` (`video_id`),
    INDEX `idx_user_id` (`user_id`),
    INDEX `idx_parent_id` (`parent_id`),
    INDEX `idx_video_created_at` (`video_id`, `created_at` DESC)
    ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci COMMENT='视频评论表';

```

## 创建评论

1. 参数校验
    1. 视频id
    2. token
    3. 操作
2. 解析token
3. 创建评论
    1. 视频是否存在（rpc调用）
    2. 根据操作创建或者删除评论
        1. 生成评论id（雪花算法）
        2. 事务创建评论
            1. 创建评论（comment）
            2. 更新评论总数（video）
            3. 删除使用软删除

## 获取视频评论列表

1. 参数校验
    1. 视频id
    2. token
2. 解析token
3. 获取视频评论列表
    1. 视频是否存在
    2. 获取视频评论列表

# relation-service

```sql
CREATE TABLE IF NOT EXISTS `relation` (
                                          `id` BIGINT UNSIGNED NOT NULL AUTO_INCREMENT COMMENT '主键ID',
                                          `user_id` BIGINT UNSIGNED NOT NULL COMMENT '用户ID',
                                          `to_user_id` BIGINT UNSIGNED NOT NULL COMMENT '关注用户的ID',
                                          `created_at` TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '关注时间',
                                          `updated_at` TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT '更新时间',
                                          `deleted_at` DATETIME DEFAULT NULL COMMENT '删除时间',
                                          PRIMARY KEY (`id`),
    UNIQUE KEY `uniq_user_to_user` (`user_id`, `to_user_id`),
    INDEX `idx_user_id` (`user_id`),
    INDEX `idx_to_user_id` (`to_user_id`),
    INDEX `idx_deleted_at` (`deleted_at`)
    ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci COMMENT='用户关系表';

```

## 关注操作

1. 参数校验
    1. 行为参数（action_type）关注：1， 取消关注：2
    2. token解析—>userid
    3. 被关注对象(to_user_id)
2. 关注逻辑
    1. to_user_id 是否存在 (rpc user-service)
    2. action_type：1
        1. 创建关注关系（user_id，to_user_id）
            1. 是否已经建立
            2. 创建关系
            3. 更新被关注用户粉丝数
            4. 更新当前用户关注数量
    3. action_type：2
        1. 删除关注关系
            1. 是否存在关系
            2. 删除关系
            3. 更新被关注用户粉丝数
            4. 更新当前用户关注数量
3. 返回响应

## 获取关注列表

1. 参数校验
    1. user_id
    2. token
2. 获取关注列表逻辑
    1. udsr_id是否存在
    2. 获取user_id关注列表
3. 返回相应

## 获取关系

1. 从redis中查询关系

1. 从es中查询关系
2. 从mysql中查询关系（兜底）

# job-service

负责kafka-elasticsearch数据流动

## kafka

1. mysql
    
    ```sql
    /etc/mysql/my.cnf
    [mysqld]
    log-bin=mysql-bin # 开启 binlog
    binlog-format=ROW # 选择 ROW 模式
    server_id=1 # 配置 MySQL replaction 需要定义，不要和 canal 的 slaveId 重复
    
    //查看
    show variables like 'log_bin'; -> on
    show variables like 'binlog_format'; -> row
    
    CREATE USER 'ysh'@'%' IDENTIFIED BY 'ysh';
    GRANT SELECT, REPLICATION SLAVE, REPLICATION CLIENT ON *.* TO 'ysh'@'%';
    FLUSH PRIVILEGES;
    ```
    
2. canal
    
    ```sql
    docker pull canal/canal-server:latest
    
    // 启动容器
    docker run -d \
      --name canal-server \
      --add-host=host.docker.internal:host-gateway \
      canal/canal-server:latest
    
    // 进入容器
    docker exec -it canal-server /bin/bash
    
    // 修该配置
    vi canal-server/conf/example/instance.properties
    
    canal.instance.master.address=host.docker.internal:3306
    
    canal.instance.tsdb.dbUsername=canal
    canal.instance.tsdb.dbPassword=password // 上面的name和password
    
    ```
    
3. kafka
    
    ```sql
    version: '2.1'
    
    services:
      zoo1:
        image: confluentinc/cp-zookeeper:7.3.2
        hostname: zoo1
        container_name: zoo1
        ports:
          - "2181:2181"
        environment:
          ZOOKEEPER_CLIENT_PORT: 2181
          ZOOKEEPER_SERVER_ID: 1
          ZOOKEEPER_SERVERS: zoo1:2888:3888
    
      tiktok-kafka:
        image: confluentinc/cp-kafka:7.3.2
        hostname: tiktok-kafka
        container_name: tiktok-kafka    # 👈 固定容器名
        ports:
          - "9092:9092"
          - "19092:19092"
          - "9999:9999"
          - "29092:29092"   # 👈 新增，供容器中其他服务（如 Canal）访问"
        environment:
          KAFKA_ADVERTISED_LISTENERS: INTERNAL://tiktok-kafka:19092,EXTERNAL://${DOCKER_HOST_IP:-127.0.0.1}:9092,DOCKER://host.docker.internal:29092
          KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT,DOCKER:PLAINTEXT
          KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL
          KAFKA_ZOOKEEPER_CONNECT: "zoo1:2181"
          KAFKA_BROKER_ID: 1
          KAFKA_LOG4J_LOGGERS: "kafka.controller=INFO,kafka.producer.async.DefaultEventHandler=INFO,state.change.logger=INFO"
          KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
          KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
          KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
          KAFKA_JMX_PORT: 9999
          KAFKA_JMX_HOSTNAME: ${DOCKER_HOST_IP:-127.0.0.1}
          KAFKA_AUTHORIZER_CLASS_NAME: kafka.security.authorizer.AclAuthorizer
          KAFKA_ALLOW_EVERYONE_IF_NO_ACL_FOUND: "true"
        depends_on:
          - zoo1
    
      kafka-ui:
        container_name: kafka-ui
        image: provectuslabs/kafka-ui:latest
        extra_hosts:
          - "host.docker.internal:host-gateway"
        ports:
          - 18080:8080
        depends_on:
          - tiktok-kafka
        environment:
          DYNAMIC_CONFIG_ENABLED: "TRUE"
    
    ```
    
4. 配置
    
    ```sql
    vi canal-server/conf/example/instance.properties
    
    canal.mq.dynamicTopic=mytest,.*,mytest.user,mytest\\..*,.*\\..* // 按需要修改
    例如：
    topic3:commit\\..* //commit是数据库
    
    // 动态配置
    
    # mq config
    canal.mq.topic=default_topic_if_no_match
    # dynamic topic route by schema or table regex
    canal.mq.flatMessage=true
    canal.mq.dynamicTopic= tiktok_users:tiktok.users,tiktok_relation:tiktok.relation
    canal.mq.partition=0
    
    # table regex
    # canal.instance.filter.regex=.*\\..*
    canal.instance.filter.regex=tiktok\\.(users|relation)
    
    vi /home/admin/canal-server/conf/canal.properties
    
    # 可选项: tcp(默认), kafka,RocketMQ,rabbitmq,pulsarmq
    canal.serverMode = kafka
    
    ##################################################
    #########                    Kafka                   #############
    ##################################################
    # 此处配置修改为你的Kafka环境地址
    kafka.bootstrap.servers = host.docker.internal:29092
    ```
    
    ```sql
    	go get github.com/segmentio/kafka-go
    ```
    

## Elasticsearch

```sql
services:
  elasticsearch:
    container_name: elasticsearch
    image: docker.elastic.co/elasticsearch/elasticsearch:8.9.1
    environment:
      - node.name=elasticsearch
      - ES_JAVA_OPTS=-Xms512m -Xmx512m
      - discovery.type=single-node
      - xpack.security.enabled=false
    ports:
      - 9200:9200
      - 9300:9300
    networks:
      - elastic
  kibana:
    image: docker.elastic.co/kibana/kibana:8.9.1
    container_name: kibana
    ports:
      - 5601:5601
    networks:
      - elastic
    depends_on:
      - elasticsearch

networks:
  elastic:
```

```sql
go get github.com/elastic/go-elasticsearch/v8@latest
```

## job-service

1. 服务配置
    
    ```sql
    
    message ElasticsearchIndex {
      string topic = 1;
      string index = 2;
    }
    
    message Elasticsearch {
      repeated string addresses = 1;
      repeated ElasticsearchIndex indices = 2;
    }
    
    message Kafka {
      repeated string brokers = 1;
      string group_id = 2;
      repeated string topics = 3;
    }
    
    elasticsearch:
      addresses:
        - "http://localhost:9200"
      indices:
        - topic: "tiktok_users"
          index: "tiktok_users"
        - topic: "tiktok_relation"
          index: "tiktok_relation"
    
    kafka:
      brokers:
        - "localhost:9092"
      group_id: "tiktok_sync_group"
      topics:
        - "tiktok_users"
        - "tiktok_relation"
    ```
    
2. job微服务
    1. 连接es，这里的es相当于一个消费者，订阅了3个topic，但每次消费都是有序的
    2. kafka
        1. broker（节点）
        2. topic（主题）
        3. group_id（消费者组）：如果我同时启动多个job微服务，就能组成一个消费者组
    3. 从kafka中读取并写入es
        1. kafka的数据的消息结构体
        2. 读取到kafka中的信息
        3. 根据给定信息拿到es中的index
        4. 反序列化
        5. 提取唯一id（幂等）
        6. 根据消息中的类型进行操作es

```sql
package job

import (
	"context"
	"encoding/json"
	"errors"
	"fmt"
	"github.com/elastic/go-elasticsearch/v8"
	"github.com/go-kratos/kratos/v2/log"
	"github.com/segmentio/kafka-go"
	"job-service/internal/conf"
)

// 消息结构体 (canal 格式)
type Msg struct {
	Type     string `json:"type"`
	Database string `json:"database"`
	Table    string `json:"table"`
	IsDdl    bool   `json:"isDdl"`
	Data     []map[string]interface{}
}

// ES 客户端封装
type EsClient struct {
	*elasticsearch.TypedClient
}

type Server interface {
	Start(context.Context) error
	Stop(context.Context) error
}

// 作业 Worker,用于消费 Kafka 消息并同步至 Elasticsearch
type JobWork struct {
	kafkaReader   *kafka.Reader
	esClient      *EsClient
	topicIndexMap map[string]string
	log           *log.Helper
}

func NewJobWrok(kafkaReader *kafka.Reader, esClient *EsClient, conf *conf.Elasticsearch, logger log.Logger) *JobWork {
	topicIndexMap := make(map[string]string)
	for _, idx := range conf.Indices {
		topicIndexMap[idx.Topic] = idx.Index
	}
	return &JobWork{
		kafkaReader:   kafkaReader,
		esClient:      esClient,
		topicIndexMap: topicIndexMap,
		log:           log.NewHelper(logger),
	}
}

// kafka
// Kafka Reader
func NewKafkaReader(c *conf.Kafka) *kafka.Reader {
	return kafka.NewReader(kafka.ReaderConfig{
		Brokers:     c.Brokers,
		GroupTopics: c.Topics,
		GroupID:     c.GroupId,
	})
}

// elsaticsearch
func NewESClient(conf *conf.Elasticsearch) (*EsClient, error) {
	// ES 配置
	c := elasticsearch.Config{Addresses: conf.Addresses}

	// 创建客户端连接
	client, err := elasticsearch.NewTypedClient(c)
	if err != nil {
		return nil, err
	}

	return &EsClient{
		TypedClient: client,
	}, nil
}

// 启动消费循环，将 canal->kafka 的变更消息同步到 Elasticsearch
func (jw JobWork) Start(ctx context.Context) error {
	jw.log.WithContext(ctx).Info("job work start")

	// 1. 从kafka中获取MySQL中的数据变更消息
	// 接收消息
	for {
		// 读取 Kafka 消息
		m, err := jw.kafkaReader.ReadMessage(ctx)
		// 如果上层 ctx 被取消，优雅退出
		if errors.Is(err, context.Canceled) {
			return nil
		}
		if err != nil {
			jw.log.Errorf("read message failed:%v\n", err)
			break
		}
		jw.log.WithContext(ctx).Infof("message at offset %d: %s = %s\n", m.Offset, string(m.Key), string(m.Value))

		// 根据当前 topic 查找对应索引
		index, ok := jw.topicIndexMap[m.Topic]
		if !ok {
			jw.log.WithContext(ctx).Errorf("no index mapping for topic: %s", m.Topic)
			continue
		}

		// 反序列化 canal 消息
		msg := new(Msg)
		if err := json.Unmarshal(m.Value, msg); err != nil {
			jw.log.WithContext(ctx).Errorf("unmarshal message failed:%v\n", err)
			continue
		}

		// 遍历变更的行数据
		for _, data := range msg.Data {
			// 提取唯一 ID（用于 ES 的文档 _id）
			docID := jw.extractID(data)
			if docID == "" {
				jw.log.WithContext(ctx).Error("missing id in message, skipping")
				continue
			}

			// 根据 canal 类型选择插入或更新
			switch msg.Type {
			case "INSERT":
				jw.indexDocument(ctx, index, docID, data)
			case "UPDATE":
				jw.updateDocument(ctx, index, docID, data)
			default:
				jw.log.WithContext(ctx).Infof("unsupported message type: %s, skipping", msg.Type)
			}
		}
	}
	return nil
}

// // 提取唯一 id，用于 ES 写入/更新时做文档 _id，便于幂等写入
func (jw *JobWork) extractID(data map[string]interface{}) string {
	if id, ok := data["id"]; ok {
		switch v := id.(type) {
		case string:
			return v
		case float64:
			// Canal 转换时可能是 float64，转回字符串
			return fmt.Sprintf("%.0f", v)
		}
	}
	return ""
}

// 在 Elasticsearch 中插入文档（幂等写入）
func (jw *JobWork) indexDocument(ctx context.Context, index, id string, data map[string]interface{}) {
	_, err := jw.esClient.Index(index).Id(id).Document(data).Do(ctx)
	if err != nil {
		jw.log.WithContext(ctx).Errorf("index document failed: %v", err)
	} else {
		jw.log.WithContext(ctx).Infof("indexed document id=%s into index=%s", id, index)
	}
}

// 在 Elasticsearch 中更新文档（幂等更新）
func (jw *JobWork) updateDocument(ctx context.Context, index, id string, data map[string]interface{}) {
	_, err := jw.esClient.Update(index, id).Doc(data).DocAsUpsert(true).Do(ctx)
	if err != nil {
		jw.log.WithContext(ctx).Errorf("update document failed: %v", err)
	} else {
		jw.log.WithContext(ctx).Infof("updated document id=%s in index=%s", id, index)
	}
}

func (jw JobWork) Stop(ctx context.Context) error {
	jw.log.WithContext(ctx).Info("job work stop")
	return jw.kafkaReader.Close()
}
```

1. 服务注册

```sql
package job

import "github.com/google/wire"

var ProviderSet = wire.NewSet(NewJobWrok, NewESClient, NewKafkaReader)

// wireApp init kratos application.
func wireApp(*conf.Server, *conf.Data, *conf.Elasticsearch, *conf.Kafka, log.Logger) (*kratos.App, func(), error) {
	panic(wire.Build(server.ProviderSet, data.ProviderSet, biz.ProviderSet, service.ProviderSet, job.ProviderSet, newApp))
}

func newApp(logger log.Logger, gs *grpc.Server, hs *http.Server, js *job.JobWork) *kratos.App {
	return kratos.New(
		kratos.ID(id),
		kratos.Name(Name),
		kratos.Version(Version),
		kratos.Metadata(map[string]string{}),
		kratos.Logger(logger),
		kratos.Server(
			//gs,
			//hs,
			js,
		),
	)
}

app, cleanup, err := wireApp(bc.Server, bc.Data, bc.Elasticsearch, bc.Kafka, logger)
```

1. es调用
    1. 对查询的数据，先到缓存中查询
    2. 缓存中没有到es中查询
        
        ```go
        resq, err := r.data.es.Search().Index(r.data.esIndex).Query(
        		&types.Query{
        			Bool: &types.BoolQuery{
        				Must: []types.Query{
        					{
        						Term: map[string]types.TermQuery{
        							"user_id": {Value: userID},
        						},
        					},
        					{
        						Term: map[string]types.TermQuery{
        							"to_user_id": {
        								Value: toUserID},
        						},
        					},
        				},
        			},
        		}).Do(ctx)
        ```
        
    3. es中没有到sql中查询（兜底）
    4. 回填缓存
2. 补充
    
    如果说我在canal中设置多个分区（如果不指定，canal会默认分给分区0），并启用hash来进行分区消息分配，然后多个job微服务，这样就组成一个消费者组来并行消费kafka。但是会导致不能保证消息的有序。但是幂等是可以保证的，而一般业务中，es只要保证幂等和消费不会漏就好了。
    

## 从es获取关系

1. 连接es
    
    ```go
    func NewEsClient(cfg *conf.Elasticsearch) (*elasticsearch.TypedClient, error) {
    	c := elasticsearch.Config{
    		Addresses: cfg.Addresses,
    	}
    	return elasticsearch.NewTypedClient(c)
    }
    
    // 加入index
    type Data struct {
    	// TODO wrapped database client
    	log *log.Helper
    	db  *gorm.DB
    	rdb *redis.Client
    
    	query   *query.Query
    	es      *elasticsearch.TypedClient
    	esIndex string
    
    	UserClient pbUser.UserServiceClient
    }
    
     esCfg *conf.Elasticsearch) 
     
     // 注入
    ```
    
2. 获取
    
    ```go
    resq, err := r.data.es.Search().Index(r.data.esIndex).Query(
    		&types.Query{
    			Bool: &types.BoolQuery{
    				Must: []types.Query{
    					{
    						Term: map[string]types.TermQuery{
    							"user_id": {Value: userID},
    						},
    					},
    					{
    						Term: map[string]types.TermQuery{
    							"to_user_id": {
    								Value: toUserID},
    						},
    					},
    				},
    			},
    		}).Do(ctx)
    	if err != nil {
    		r.log.WithContext(ctx).Errorf("queryRelationExistInES es search error: %v", err)
    		return false, err
    	}
    	return resq.Hits.Total.Value > 0, nil
    ```
    

## 从es模糊获取视频

1. 连接es，注入
2. 模糊搜索

```go
func (r *videoRepo) GetVideoByTitle(ctx context.Context, title string) ([]*v1.Video, error) {
	r.log.WithContext(ctx).Infof("GetVideoByTitle: %v", title)
	r.log.WithContext(ctx).Debugf("esIndex: %s", r.data.esIndex)
	// 1. 在es中模糊查询
	res, err := r.data.es.Search().
		Index(r.data.esIndex).
		Query(
			&types.Query{
				Match: map[string]types.MatchQuery{
					"title": {Query: title},
				},
			},
		).
		Size(20).
		Do(ctx)
	if err != nil {
		r.log.WithContext(ctx).Errorf("get video err: %v", err)
		// 2️⃣ 兜底用 SQL
		return r.getVideoByTitleFromDB(ctx, title)
	}
	if res == nil || res.Hits.Hits == nil || len(res.Hits.Hits) == 0 {
		r.log.WithContext(ctx).Warnf("ES no hits for title: %s, fallback to DB", title)
		return r.getVideoByTitleFromDB(ctx, title)
	}

	// 2. 在sql中模糊查询兜底

	videos := make([]*v1.Video, 0, len(res.Hits.Hits))
	for _, hit := range res.Hits.Hits {
		var video v1.Video
		if err := json.Unmarshal(hit.Source_, &video); err != nil {
			r.log.WithContext(ctx).Errorf("unmarshal video err: %v, source: %s", err, string(hit.Source_))
			continue
		}
		videos = append(videos, &video)
	}

	if len(videos) == 0 {
		return r.getVideoByTitleFromDB(ctx, title)
	}

	return videos, nil
}
```

# redis缓存

1. 缓存用户信息
2. 视频按分数获取
    1. 创建视频初始化视频分数
    2. 更新评论更新视频分数
    3. 更新点赞更新视频分数
    4. 获取视频
        1. 按分数排序到缓存中获取视频id
        2. 根据视频id到数据库中查找视频信息
        3. 按分数排序返回视频

# prometheus监控

1. 配置
    
    ```go
    global:
      scrape_interval: 15s
      evaluation_interval: 15s
    
    scrape_configs:
      - job_name: "prometheus"
        static_configs:
          - targets: ["localhost:19091"]
    
      - job_name: "users-service"
        metrics_path: "/metrics"
        static_configs:
          - targets: ["host.docker.internal:8080"]
    
    docker volume create prometheus-data
    
    docker run -d --name prometheus `
      -p 19091:9090 `
      -v D:/my_program/goland/tiktok/pkg/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml `
      -v prometheus-data:/prometheus `
      prom/prometheus
    
    ```
    
2. 提供metric
    1. 定义要监控的
        1. 请求次数
        2. 请求耗时
        3. 请求错误率
        4. db查询耗时和错误率
        5. 内存占用率
        6. cpu
        7. goroutine
        8. gc
        9. golang runtime
        10. kafka
        
        等
        
    2. 开启一个独立的metric服务，为prometheus提供
        
        ```go
        func StartMetricsServer() {
        	http.Handle("/metrics", promhttp.Handler())
        	log.Println("Prometheus metrics exposed at :18081/metrics")
        	go func() {
        		if err := http.ListenAndServe(":18081", nil); err != nil {
        			log.Fatalf("Metrics server failed: %v", err)
        		}
        	}()
        }
        ```
        
    3. http执行中间件
        
        ```go
        
        // statusResponseWriter 用来记录返回状态码
        type statusResponseWriter struct {
        	http.ResponseWriter
        	statusCode int
        }
        
        func (rw *statusResponseWriter) WriteHeader(code int) {
        	rw.statusCode = code
        	rw.ResponseWriter.WriteHeader(code)
        }
        
        // 请求数量和处理时间
        // InstrumentHandler wraps your Kratos HTTP handler to collect Prometheus metrics, InstrumentHandler 是核心中间件
        func InstrumentHandler(next http.Handler) http.Handler {
        	return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
        		start := time.Now()
        		rw := &statusResponseWriter{ResponseWriter: w, statusCode: 200}
        
        		// 执行业务逻辑
        		next.ServeHTTP(rw, r)
        		duration := time.Since(start).Seconds()
        
        		path := r.URL.Path
        		method := r.Method
        		status := http.StatusText(rw.statusCode)
        
        		RequestCount.WithLabelValues(path, method, status).Inc()
        		RequestDuration.WithLabelValues(path, method, status).Observe(duration)
        		if rw.statusCode >= 400 {
        			HTTPErrorCount.WithLabelValues(path, method, status).Inc()
        		}
        	})
        }
        ```
        
    4. 加入到http
        
        ```go
        var opts = []http.ServerOption{
        		http.Middleware(
        			recovery.Recovery(),
        		),
        		http.Filter(metrics.InstrumentHandler),
        	}
        ```
        
    5. sql的监控
        
        ```go
        done := ObserveDuration(metrics.DBQueryDuration, []string{"CheckUserExistByUsername"})
        	_, err := r.data.query.User.
        		WithContext(ctx).
        		Where(r.data.query.User.Username.Eq(userName)).
        		First()
        	done()
        	if err != nil {
        		if err == gorm.ErrRecordNotFound {
        			log.Debugf("user %s not exist", userName)
        			return false, nil
        		}
        		metrics.DBQueryErrorCount.WithLabelValues("CheckUserExistByUsername", err.Error()).Inc()
        		return false, err
        	}
        	
        	
        	// ObserveDuration Prometheus 监控
        func ObserveDuration(histogram *prometheus.HistogramVec, labels []string) func() {
        	start := time.Now()
        	return func() {
        		histogram.WithLabelValues(labels...).Observe(time.Since(start).Seconds())
        	}
        }
        ```
        

## grafana

1. 拉取
    
    ```go
    docker run -d --name=grafana --add-host=host.docker.internal:host-gateway -p 3000:3000 grafana/grafana-oss
    ```
    
2. 配置prometheus数据源
    
    Connections→prometheus →Add new data source→url(prometheus)→Save & Test
    
3. 仪表板
    
    Dashboards→Create Dashboard→Add visualization/import(json)→prometheus data source
    
4. 输入表达式
    
    ```go
    user_service_requests_total,user_service_request_duration_seconds_bucket,go_goroutines
    go_memstats_alloc_bytes,process_cpu_seconds_total...
    ```
    
5. save

# 压测

### wrk

```lua
wrk.method = "GET"
wrk.headers["Content-Type"] = "application/json"

paths = {
	 "/api/user/check?user_id=23359767250468865"
}

request = function()
  local path = paths[math.random(1, #paths)]
  return wrk.format(nil, path)
end

wrk -t4 -c100 -d30s -s multi_api.lua http://127.0.0.1:8080
```

也可以用docker

```lua
docker run --rm williamyeh/wrk -t4 -c1000 -d30s http://host.docker.internal:8080/api/user?user_id=23359767250468865
```

# OpenTelemetry

1. 依赖配置
    
    ```go
    go get "go.opentelemetry.io/otel" \
      "go.opentelemetry.io/otel/exporters/stdout/stdoutmetric" \
      "go.opentelemetry.io/otel/exporters/stdout/stdouttrace" \
      "go.opentelemetry.io/otel/propagation" \
      "go.opentelemetry.io/otel/sdk/metric" \
      "go.opentelemetry.io/otel/sdk/resource" \
      "go.opentelemetry.io/otel/sdk/trace" \
      "go.opentelemetry.io/otel/semconv/v1.24.0" \
      "go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp"
    ```
    
2. 初始化
    1. 定义资源`用于标识服务信息，如 service.name, environment 等`
        
        ```go
        res, err := resource.New(ctx,
        		resource.WithAttributes(
        			semconv.ServiceNameKey.String(serviceName), // 设置服务名称
        			attribute.String("env", "dev"),             // 可根据需要设置环境: dev/staging/prod
        		),
        	)
        ```
        
    2. 创建出口，用于链路追踪的jaeger可视化
        
        ```go
        exporter, err := otlptracegrpc.New(ctx,
        		otlptracegrpc.WithInsecure(),
        		otlptracegrpc.WithEndpoint("localhost:4317"), // Jaeger or Tempo OTLP gRPC endpoint
        		otlptracegrpc.WithDialOption(grpc.WithBlock()),
        	)
        ```
        
    3. 批量发送给jaeger
        
        ```go
        // 创建 BatchSpanProcessor，用于批量发送 Trace 数据（提高性能）
        	bsp := sdktrace.NewBatchSpanProcessor(exporter)
        ```
        
    4. 创建TraceProvider
        
        ```go
        // 创建 TracerProvider，并设置资源信息及 BatchSpanProcessor
        	tp := sdktrace.NewTracerProvider(
        		sdktrace.WithResource(res),
        		sdktrace.WithSpanProcessor(bsp),
        	)
        ```
        
    5. 全局设置
        
        ```go
        // 设置全局 TracerProvider，之后通过 otel.Tracer("") 获取的 Tracer 即可使用
        	otel.SetTracerProvider(tp)
        ```
        
    6. 上下文传播器，分布式跨链路传递trace_id
        
        ```go
        // 设置全局上下文传播器，TraceContext 用于分布式链路跨服务传递 trace id
        	otel.SetTextMapPropagator(propagation.TraceContext{})
        ```
        
    7. 关闭
        
        ```go
        // 返回 shutdown 函数用于优雅关闭（Kratos shutdown 时调用，避免丢失数据）
        	return tp.Shutdown
        ```
        
3. 项目中使用
    1. 初始化
        
        ```go
        	// main.go
        	// ---------------OpenTelemetry--------------------
        	ctx := context.Background()
        	shutdown := otelsetup.InitTracerProvider(ctx, Name)
        	defer func() {
        		ctx, cancel := context.WithTimeout(ctx, time.Second*5)
        		defer cancel()
        		if err := shutdown(ctx); err != nil {
        			log.Fatalf("failed to shutdown tracer: %v", err)
        		}
        	}()
        	
        	
        ```
        
    2. grpc传递
        
        ```go
        gogrpc "google.golang.org/grpc" 
        "go.opentelemetry.io/contrib/instrumentation/google.golang.org/grpc/otelgrpc"
        // 服务端口
        var opts = []grpc.ServerOption{
        		grpc.Middleware(
        			recovery.Recovery(),
        		),
        		grpc.Options(
        			gogrpc.StatsHandler(otelgrpc.NewServerHandler()),
        		),
        	}
        	
        // clent端
        	connUser, err := grpc.DialInsecure(
        		ctx,
        		grpc.WithEndpoint(c.UserService.Endpoint),
        		grpc.WithDiscovery(rr),
        		grpc.WithOptions(gogrpc.WithStatsHandler(otelgrpc.NewClientHandler())),
        	)
        ```
        
    3. 逻辑打点
        1. 父span
            
            ```go
            // service
            // opentelemetry
            	ctx, span := tracing.StartSpan(ctx, "CommentService.CreateComment",
            		attribute.String("comment.video_id", strconv.FormatInt(in.VideoId, 10)),
            	)
            	defer span.End()
            ```
            
        2. 子span，在grpc调用，sql，redis等调用地方打span
            
            ```go
            // 1.grpc
            ctx, span := tracing.StartSpan(ctx, "commentRepo.ParseToken",
            		attribute.String("token.length", fmt.Sprintf("%d", len(token))),
            	)
            	defer span.End()
            
            // 2. redis
            	keyVideoComment := fmt.Sprintf("video:comment:%d", req.VideoId)
            
            	ctxCache, spanCache := tracing.StartSpan(ctx, "Redis.CheckCache")
            	if err = c.checkVideoCommentInCache(ctxCache, keyVideoComment, req.VideoId); err != nil {
            		spanCache.RecordError(err)
            		spanCache.End()
            		return nil, err
            	}
            	spanCache.End()
            ```
            
        3. 被调用方也需要打点
        4. 封装tracing,sapn
            
            ```go
            // StartSpan starts and returns a span with context for more flexible usage.
            func StartSpan(ctx context.Context, spanName string, attrs ...attribute.KeyValue) (context.Context, trace.Span) {
            	tracer := otel.Tracer("app-tracer")
            	ctx, span := tracer.Start(ctx, spanName)
            	if len(attrs) > 0 {
            		span.SetAttributes(attrs...)
            	}
            	return ctx, span
            }
            
            // TraceFunc wraps a function with a span, reducing boilerplate.
            func TraceFunc(ctx context.Context, spanName string, fn func(ctx context.Context) error) error {
            	tracer := otel.Tracer("app-tracer")
            	ctx, span := tracer.Start(ctx, spanName)
            	defer span.End()
            	return fn(ctx)
            }
            ```
            

## jaeger

```go
docker run -d --name jaeger \
  -e COLLECTOR_OTLP_ENABLED=true \
  -e COLLECTOR_ZIPKIN_HTTP_PORT=9411 \
  -p 5775:5775/udp \
  -p 6831:6831/udp \
  -p 6832:6832/udp \
  -p 5778:5778 \
  -p 16686:16686 \
  -p 14268:14268 \
  -p 14250:14250 \
  -p 4317:4317 \            # 启用 gRPC
  -p 4318:4318 \            # 可选：HTTP
  -p 9411:9411 \
  jaegertracing/all-in-one:latest
```

## 配置到granan

# zap日志

1. 配置
    
    ```go
    message Log {
      string level = 1;
      string path = 2;
      int32 max_size = 3;
      int32 max_backups = 4;
      int32 max_age = 5;
      bool compress = 6;
      bool console = 7;
    }
    
    log:
      level: debug
      path: ../logs/app.log
      max_size: 100
      max_backups: 5
      max_age: 3
      compress: true
      console: true
    ```
    
2. 封装
    
    ```go
    // zapLogger 实现 Kratos log.Logger 接口，封装 zap.SugaredLogger
    // 用于在 Kratos 中无侵入替换日志，支持控制台打印与文件落盘
    // 通过 zapcore、lumberjack 实现按大小切割、保留历史、压缩归档
    type zapLogger struct {
    	log *zap.SugaredLogger
    }
    
    // Log 实现 Kratos log.Logger 接口的方法，将 Kratos Level 映射到 zap 方法
    func (l *zapLogger) Log(level log.Level, keyvals ...interface{}) error {
    	switch level {
    	case log.LevelDebug:
    		l.log.Debugw("", keyvals...)
    	case log.LevelInfo:
    		l.log.Infow("", keyvals...)
    	case log.LevelWarn:
    		l.log.Warnw("", keyvals...)
    	case log.LevelError:
    		l.log.Errorw("", keyvals...)
    	default:
    		l.log.Infow("", keyvals...)
    	}
    	return nil
    }
    
    type LogConfig struct {
    	Level      string `yaml:"level"`
    	Path       string `yaml:"path"`
    	MaxSize    int    `yaml:"max_size"`
    	MaxBackups int    `yaml:"max_backups"`
    	MaxAge     int    `yaml:"max_age"`
    	Compress   bool   `yaml:"compress"`
    	Console    bool   `yaml:"console"`
    }
    
    // NewZapLogger 创建可直接用于 Kratos log.With 的 zapLogger 实例
    // 落盘到 logs/app.log（JSON），控制台默认 io.Discard，如需打印替换为 os.Stdout
    func NewZapLogger(cfg LogConfig) log.Logger {
    	encoderConfig := zap.NewProductionEncoderConfig()
    	encoderConfig.EncodeTime = zapcore.ISO8601TimeEncoder
    
    	fileWriter := zapcore.AddSync(&lumberjack.Logger{
    		Filename:   cfg.Path,
    		MaxSize:    cfg.MaxSize,    // 单文件最大 100MB
    		MaxBackups: cfg.MaxBackups, // 最多保留 10 个历史文件
    		MaxAge:     cfg.MaxAge,     // 文件最大保存天数
    		Compress:   cfg.Compress,   // 是否压缩归档
    	})
    
    	var consoleWriter zapcore.WriteSyncer = zapcore.AddSync(io.Discard)
    	if cfg.Console {
    		consoleWriter = zapcore.AddSync(os.Stdout)
    	}
    
    	level := zap.InfoLevel
    	switch strings.ToLower(cfg.Level) {
    	case "debug":
    		level = zap.DebugLevel
    	case "info":
    		level = zap.InfoLevel
    	case "warn", "warning":
    		level = zap.WarnLevel
    	case "error":
    		level = zap.ErrorLevel
    	}
    
    	core := zapcore.NewCore(
    		zapcore.NewJSONEncoder(encoderConfig),
    		zapcore.NewMultiWriteSyncer(fileWriter, consoleWriter),
    		level,
    	)
    
    	logger := zap.New(core, zap.AddCaller(), zap.AddCallerSkip(1)).Sugar()
    	return &zapLogger{log: logger}
    }
    
    var _ log.Logger = (*zapLogger)(nil)
    
    ```
    
3. 注入
    
    ```go
    	var logCfg pkg.LogConfig
    	if err := c.Scan(&logCfg); err != nil {
    		panic(err)
    	}
    	logger := log.With(pkg.NewZapLogger(logCfg),
    		//"ts", log.DefaultTimestamp,
    		//"caller", log.DefaultCaller,
    		"service.id", id,
    		"service.name", Name,
    		"service.version", Version,
    		"trace.id", tracing.TraceID(),
    		"span.id", tracing.SpanID(),
    	)
    
    ```
    

# 限流，熔断

1. 限流（令牌桶）
    
    ```go
    import (
    	"context"
    	"errors"
    	"github.com/go-kratos/kratos/v2/middleware"
    	"golang.org/x/time/rate"
    )
    
    // RateLimitMiddleware 返回一个限流中间件，基于令牌桶算法。
    // qps 是允许的请求速率，burst 是允许的突发容量。
    func RateLimitMiddleware(qps float64, burst int) middleware.Middleware {
    	limiter := rate.NewLimiter(rate.Limit(qps), burst)
    
    	return func(next middleware.Handler) middleware.Handler {
    		return func(ctx context.Context, req interface{}) (interface{}, error) {
    			// Allow 允许立即获取一个令牌，如果获取不到直接返回限流错误。
    			if !limiter.Allow() {
    				return nil, errors.New("too many requests - rate limit exceeded")
    			}
    			return next(ctx, req)
    		}
    	}
    }
    
    ```
    
2. 熔断
    
    ```go
    import (
    	"github.com/sony/gobreaker"
    	"time"
    )
    
    // CircuitBreaker 包装了 sony/gobreaker.CircuitBreaker，方便复用。
    type CircuitBreaker struct {
    	cb *gobreaker.CircuitBreaker
    }
    
    // NewCircuitBreaker 创建一个新的熔断器，name 标识熔断器名称。
    func NewCircuitBreaker(name string) *CircuitBreaker {
    	settings := gobreaker.Settings{
    		Name:        name,
    		MaxRequests: 5,                // 半开状态最大允许请求数
    		Interval:    60 * time.Second, // 统计窗口周期，计数清零
    		Timeout:     30 * time.Second, // 熔断打开后尝试恢复时间
    		ReadyToTrip: func(counts gobreaker.Counts) bool {
    			// 失败率超过60%，且请求数至少10个时打开熔断
    			failureRatio := float64(counts.TotalFailures) / float64(counts.Requests)
    			return counts.Requests >= 10 && failureRatio >= 0.6
    		},
    	}
    	return &CircuitBreaker{
    		cb: gobreaker.NewCircuitBreaker(settings),
    	}
    }
    
    // Execute 执行函数，如果熔断器打开则直接返回错误。
    // 函数内部需要返回 (interface{}, error)，
    func (c *CircuitBreaker) Execute(req func() (interface{}, error)) (interface{}, error) {
    	return c.cb.Execute(req)
    }
    
    ```
    

<aside>
💡

限流就是控制访问服务的请求速率，避免瞬时流量过大压垮服务。

**熔断**的作用稍微不同，它是保护服务在出现故障时“快速失败”和“自动恢复”的机制，防止故障扩大，避免雪崩效应。具体来说：

- 当调用某个下游服务时，如果它连续出现失败（比如响应超时、错误率过高等），熔断器就会“打开”。
- 打开后，后续调用不会再发到这个故障服务，而是立刻返回错误，快速失败，节省资源。
- 经过一段时间（熔断器的重试间隔），熔断器会进入“半开”状态，允许少量请求尝试调用服务，看服务是否恢复。
- 如果恢复正常，熔断器“关闭”，恢复正常调用；否则继续保持“打开”，拒绝请求。

这样做的好处是：

- 避免客户端无限制等待一个已经不可用的服务，提高系统响应速度和稳定性。
- 给下游服务“喘息”时间，减少连锁故障扩散。
- 整体提升微服务系统的健壮性和容错能力。
</aside>

| 功能 | 限流 | 熔断 |
| --- | --- | --- |
| 目的 | 控制访问频率，防止流量暴涨 | 保护调用方，快速失败和恢复 |
| 作用对象 | 服务入口请求 | 下游服务调用 |
| 触发条件 | 请求速率超过设定阈值 | 下游服务连续失败率过高 |
| 处理方式 | 拒绝超出速率的请求 | 拒绝调用故障服务，快速返回错误 |
1. 使用

```go
// 创建限流中间件，限制全局入口请求
var RateLimitMw = middleware.RateLimitMiddleware(1000, 1888) // 100 QPS，突发200

var opts = []http.ServerOption{
		http.Middleware(
			recovery.Recovery(),
			RateLimitMw,
		),
	}

	grpc.Middleware(
			recovery.Recovery(),
			RateLimitMw,
		),
		

```

1. 逻辑中使用
    
    ```go
    var (
    	// 全局限流器，10 QPS，burst 20
    	commentRateLimiter = middleware.RateLimitMiddleware(10, 20)
    
    	// comment -> user 服务调用熔断器
    	userParseTokenCB = middleware.NewCircuitBreaker("user-parse-token")
    )
    
    func (c *commentRepo) ParseToken(ctx context.Context, token, refreshToken string) (int64, error) {
    
    	exec := func(ctx context.Context) (interface{}, error) {
    		ctx, span := tracing.StartSpan(ctx, "commentRepo.ParseToken",
    			attribute.String("token.length", fmt.Sprintf("%d", len(token))),
    		)
    		defer span.End()
    
    		return userParseTokenCB.Execute(func() (interface{}, error) {
    			return c.data.UserClient.ParseToken(ctx, &pbUser.ParseTokenRequest{
    				Token:        token,
    				RefreshToken: refreshToken,
    			})
    		})
    	}
    
    	result, err := commentRateLimiter(func(ctx context.Context, req interface{}) (interface{}, error) {
    		return exec(ctx)
    	})(ctx, nil)
    
    	if err != nil {
    		return 0, err
    	}
    
    	resp, ok := result.(*pbUser.ParseTokenReply)
    	if !ok || resp == nil {
    		return 0, errors.New("failed to parse token")
    	}
    
    	return resp.UserId, nil
    }
    
    ```
    

# git

1. 推送
    1. git checkout -b feature/xxx
    2. git add xxxxx
    3. git status
    4.  git commit -m “xxxx”
    5. git push origin feature/xxx
2. 合并
    1. git checkout develop
    2. git branch —> develop
    3. git push origin develop
    4. git merge feature/xxx
    5. 合并main

# Docker

> docker build -t youngking666/xxxx-service:latest .

>docker run -d --name xxxx-service-test -p 8081:8081  -p 908x:908x youngking666/xxxx-service:latest

> docker push youngking666/xxxx-service:latest

这里新建一个网络，并且将容器放入到一个网络中

docker network create tiktok-network

dokcer network connect tiktok-network youngking666/xxxx-service:latest

docker network inspect  tiktok-network

这样在项目中的配置文件的各个容器的地址可以直接改为

contanier-name:port

# consul

1. 服务注册
    
    ```go
    func NewRegistry(cfg *conf.Registry) registry.Registrar {
    	c := api.DefaultConfig()
    	c.Address = cfg.Consul.Addr
    	c.Scheme = cfg.Consul.Scheme
    	client, err := api.NewClient(c)
    	if err != nil {
    		panic(err)
    	}
    
    	reg := consul.New(client, consul.WithHealthCheck(true))
    	return reg
    }
    
    func newApp(logger log.Logger, gs *grpc.Server, hs *http.Server, reg registry.Registrar) *kratos.App {
    	return kratos.New(
    		kratos.ID(id),
    		kratos.Name(Name),
    		kratos.Version(Version),
    		kratos.Metadata(map[string]string{}),
    		kratos.Logger(logger),
    		kratos.Server(
    			gs,
    			hs,
    		),
    		kratos.Registrar(reg),
    	)
    }
    ```
    
2. 服务发现
    
    ```go
    func NewDiscover(cfg *conf.Registry) registry.Discovery {
    	// new consul client
    	c := api.DefaultConfig()
    	c.Address = cfg.Consul.Addr
    	c.Scheme = cfg.Consul.Scheme
    	client, err := api.NewClient(c)
    	if err != nil {
    		panic(err)
    	}
    	// new dis with consul client
    	reg := consul.New(client)
    	return reg
    }
    
    func NewUserServiceClient(c *conf.Data, rr registry.Discovery) pbUser.UserServiceClient {
    	conn, err := grpc.DialInsecure(
    		context.Background(),
    		grpc.WithEndpoint(c.UserService.Endpoint),
    		grpc.WithDiscovery(rr),
    		grpc.WithOptions(gogrpc.WithStatsHandler(otelgrpc.NewClientHandler())),
    	)
    	if err != nil {
    		panic(err)
    	}
    	return pbUser.NewUserServiceClient(conn)
    }
    ```
    

# docker-compose

1. mysql 数据配置迁移
    1. 数据库
        
        ```go
        mysqldump -u root -p --databases tiktok > tiktok_backup.sql
        
        // 找到tiktok_backup.sql并保存
        
        // 复制到容器中
        // docker cp D:\my_program\goland\tiktok\tiktok_backup.sql new_mysql_container:/tiktok_backup.sql
        
        // 执行
        // docker exec -it new_mysql_container bash
        // mysql -u root -p < /tiktok_backup.sql
        我这里没有用这种方式，用的是在docker-compose中直接导入。
        里面有坑！！！
        导出的 tiktok_backup.sql 中有
        1. create use 这里可能会导致出错，如果你已经有有这个数据库了，就删除 
        2. 换行符的问题！！！
        		**dos2unix ./mysql/tiktok_backup.sql**
        一定要修改换行符，可能是我是windows的原因，最开始没有换行符，一直出错
        3. 文件权限的问题，修改为644
        ```
        
    2. 配置文件
        
        ```go
        mysql --help | grep my.cnf
        
                              order of preference, my.cnf, $MYSQL_TCP_PORT,
        /etc/my.cnf /etc/mysql/my.cnf /usr/etc/my.cnf ~/.my.cnf 
        
        // 找到/etc/my.cnf保存，这个是我之前修改的。记得加权限
        
        // 挂载
            volumes:
              - ./my.cnf:/etc/my.cnf
              - mysql_data:/var/lib/mysql
        ```
        
2. canal配置文件
    
    ```go
    这里有几个休要修改的，坑！！！
    1. meta.dat 删除，不然会报错的。
    2. 修改 ./canal-conf/canal.properties 和instance中配置，docker.host.internal 改为mysql 和kafka
    2.1 配置文件中的29092也可以不要了，这里需要修改canal.properties 哪里的mysql也改为19092
    3. 先试试上面删除了是否能行，如果不能行，就接下来
    3. ./canal-conf/canal.properties
    		canal.instance.parser=com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser
    ```
    
3. 启动或删除
    
    ```go
    docker-compose down -v --remove-orphans
    
    	
    docker-compose build --no-cache  # 可选，如果你想强制重新构建镜像
    docker-compose up -d
    ```
    

# k8s

```go
[root@master ~]# kubectl cluster-info
Kubernetes master is running at https://192.168.198.100:6443
KubeDNS is running at https://192.168.198.100:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy

To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.

[root@master ~]# kubectl get nodes
NAME     STATUS   ROLES    AGE   VERSION
master   Ready    master   50d   v1.17.4
node1    Ready    <none>   50d   v1.17.4
node2    Ready    <none>   50d   v1.17.4

[root@master ~]# kubectl get storageclass
No resources found in default namespace.

[root@master ~]# kubectl get storageclass -n tiktok
No resources found in tiktok namespace.

[root@master ~]# vim local-storageclass.yaml
[root@master ~]# kubectl apply -f local-storageclass.yaml

		apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: local-storage
provisioner: kubernetes.io/no-provisioner
volumeBindingMode: WaitForFirstConsumer

storageclass.storage.k8s.io/local-storage created
[root@master ~]# kubectl get storageclass -n tiktok
NAME            PROVISIONER                    RECLAIMPOLICY   VOLUMEBINDINGMODE      ALLOWVOLUMEEXPANSION   AGE
local-storage   kubernetes.io/no-provisioner   Delete          WaitForFirstConsumer   false                  16s
[root@master ~]#

[root@master ~]# kubectl get storageclass -n tiktok
NAME            PROVISIONER                    RECLAIMPOLICY   VOLUMEBINDINGMODE      ALLOWVOLUMEEXPANSION   AGE
local-storage   kubernetes.io/no-provisioner   Delete          WaitForFirstConsumer   false                  16s

[root@master ~]# vim pv.yaml
	apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-mysql
spec:
  capacity:
    storage: 10Gi
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  storageClassName: local-storage
  local:
    path: /data/mysql
  nodeAffinity:
    required:
      nodeSelectorTerms:
      - matchExpressions:
        - key: kubernetes.io/hostname
          operator: In
          values:
          - node1
---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-redis
spec:
  capacity:
    storage: 10Gi
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  storageClassName: local-storage
  local:
    path: /data/redis
  nodeAffinity:
    required:
      nodeSelectorTerms:
      - matchExpressions:
        - key: kubernetes.io/hostname
          operator: In
          values:
          - node1
		
[root@master ~]# kubectl apply -f pv.yaml
persistentvolume/pv-mysql created
persistentvolume/pv-redis created
[root@master ~]# kubectl get storageclass
NAME            PROVISIONER                    RECLAIMPOLICY   VOLUMEBINDINGMODE      ALLOWVOLUMEEXPANSION   AGE
local-storage   kubernetes.io/no-provisioner   Delete          WaitForFirstConsumer   false                  5m58s
[root@master ~]# kubectl get pv
NAME       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM   STORAGECLASS    REASON   AGE
pv-mysql   10Gi       RWO            Retain           Available           local-storage            16s
pv-redis   10Gi       RWO            Retain           Available           local-storage            16s
[root@master ~]# ^C
[root@master ~]# mkdir tiktok
[root@master ~]# vim pv.yaml 
	apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-kafka
spec:
  capacity:
    storage: 10Gi
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  storageClassName: local-storage
  local:
    path: /data/kafka
  nodeAffinity:
    required:
      nodeSelectorTerms:
      - matchExpressions:
        - key: kubernetes.io/hostname
          operator: In
          values:
          - node1
---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-minio
spec:
  capacity:
    storage: 10Gi
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  storageClassName: local-storage
  local:
    path: /data/minio
  nodeAffinity:
    required:
      nodeSelectorTerms:
      - matchExpressions:
        - key: kubernetes.io/hostname
          operator: In
          values:
          - node1
---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-es
spec:
  capacity:
    storage: 10Gi
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  storageClassName: local-storage
  local:
    path: /data/es
  nodeAffinity:
    required:
      nodeSelectorTerms:
      - matchExpressions:
        - key: kubernetes.io/hostname
          operator: In
          values:
          - node1
---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-prometheus
spec:
  capacity:
    storage: 10Gi
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  storageClassName: local-storage
  local:
    path: /data/prometheus
  nodeAffinity:
    required:
      nodeSelectorTerms:
      - matchExpressions:
        - key: kubernetes.io/hostname
          operator: In
          values:
          - node1
---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-grafana
spec:
  capacity:
    storage: 10Gi
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  storageClassName: local-storage
  local:
    path: /data/grafana
  nodeAffinity:
    required:
      nodeSelectorTerms:
      - matchExpressions:
        - key: kubernetes.io/hostname
          operator: In
          values:
          - node1
ssh node1
[root@node1 data]# mkdir -p /data/kafka /data/minio /data/es /data/prometheus /data/grafana
[root@node1 data]# chmod 777 /data/kafka /data/minio /data/es /data/prometheus /data/grafana

[root@master ~]# kubectl get pv
NAME            CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM   STORAGECLASS    REASON   AGE
pv-es           1Gi        RWO            Retain           Available           local-storage            16h
pv-grafana      1Gi        RWO            Retain           Available           local-storage            16h
pv-kafka        1Gi        RWO            Retain           Available           local-storage            16h
pv-minio        1Gi        RWO            Retain           Available           local-storage            16h
pv-mysql        2Gi        RWO            Retain           Available           local-storage            16h
pv-prometheus   1Gi        RWO            Retain           Available           local-storage            16h
pv-redis        10Gi       RWO            Retain           Available           local-storage            16h

#####################################
上传配置文件，转换配置文件
# 安装 dos2unix
yum install dos2unix  # CentOS
# 或
apt-get update && apt-get install dos2unix  # Ubuntu

# 转换文件
cd /root/config
dos2unix ./mysql/my.cnf ./mysql/init.sql ./mysql/tiktok_backup.sql ./prometheus.yml
find ./canal-conf -type f -exec dos2unix {} \;

# 验证
file ./mysql/my.cnf
cat -v ./mysql/my.cnf | head -n 5
###########################################

kubectl create configmap mysql-config \
  --from-file=./mysql/my.cnf \
  --from-file=./mysql/init.sql \
  --from-file=./mysql/tiktok_backup.sql \
  -n tiktok

kubectl create configmap canal-config \
  --from-file=./canal-conf \
  -n tiktok

kubectl create configmap prometheus-config \
  --from-file=./prometheus.yml \
  -n tiktok
  
 kubectl create secret generic mysql-secret \
  --from-literal=root-password=root \
  --from-literal=database=root\
  -n tiktok

kubectl create secret generic canal-secret \
  --from-literal=canal-username=ysh \
  --from-literal=canal-password=ysh \
  -n tiktok

kubectl create secret generic minio-secret \
  --from-literal=access-key=admin \
  --from-literal=secret-key=admin123 \
  -n tiktok

kubectl create secret generic grafana-secret \
  --from-literal=admin-password=admin \
  -n tiktok
  
  下载kompose
  curl -L https://github.com/kubernetes/kompose/releases/download/v1.22.0/kompose-linux-amd64 -o kompose
  也可以直接下载到本地后上传到服务器
  
	chmod +x kompose-linux-amd64
	sudo mv kompose-linux-amd64 /usr/local/bin/kompose
	kompose version
	
[root@master config]# kompose version
1.22.0 (955b78124)
[root@master config]# file /usr/local/bin/kompose
/usr/local/bin/kompose: ELF 64-bit LSB executable, x86-64, version 1 (SYSV), statically linked, stripped
[root@master config]# /usr/local/bin/kompose version
1.22.0 (955b78124)

修改docker-compose.yaml，因为不支持network等，需要修改
[root@master config]# mkdir kubernetes-manifests
[root@master config]# kompose convert -o kubernetes-manifests/
[root@master config]# ls kubernetes-manifests/

修改生产的配置文件的development, service， data-pvc
[root@master kubernetes-manifests]# vim mysql-deployment.yaml
[root@master kubernetes-manifests]# vim mysql-service.yaml
[root@master kubernetes-manifests]# vim mysql-data-persistentvolumeclaim.yaml

// 创建
[root@master kubernetes-manifests]# kubectl apply -f mysql-deployment.yaml -f mysql-service.yaml -f mysql-data-persistentvolumeclaim.yaml -             n tiktok

// 查看
[root@master kubernetes-manifests]# kubectl get pods -n tiktok -o wide
[root@master kubernetes-manifests]# kubectl get svc -n tiktok
[root@master kubernetes-manifests]# kubectl get pvc -n tiktok

```

## 1. namespace

```yaml
apiVersion: v1
kind: Namespace
metadata:
  name: tiktok
```

## 2. mysql

1. configmap
    
    ```go
    # 注意路径要准确
    kubectl create configmap mysql-config \
      --from-file=my.cnf=./mysql/my.cnf \
      -n tiktok
    
    kubectl create configmap mysql-init-sql \
      --from-file=1_init.sql=./mysql/init.sql \
      --from-file=2_tiktok_backup.sql=./mysql/tiktok_backup.sql \
      -n tiktok
    
    ```
    
2. pv
    
    ```yaml
    # mysql-pv.yaml
    apiVersion: v1
    kind: PersistentVolume
    metadata:
      name: mysql-pv
    spec:
      capacity:
        storage: 10Gi
      accessModes:
        - ReadWriteOnce
      persistentVolumeReclaimPolicy: Retain
      hostPath:
        path: /data/mysql
      nodeAffinity:
        required:
          nodeSelectorTerms:
            - matchExpressions:
                - key: kubernetes.io/hostname
                  operator: In
                  values:
                    - node2
    
    ssh node2
    sudo mkdir -p /data/mysql
    sudo chown -R 999:999 /data/mysql   # 999 是 mysql 容器用户（mysql:8 默认）
    ```
    
3. pvc
    
    ```yaml
    
    # mysql-pvc.yaml
    apiVersion: v1
    kind: PersistentVolumeClaim
    metadata:
      name: mysql-pvc
      namespace: tiktok
    spec:
      accessModes:
        - ReadWriteOnce
      resources:
        requests:
          storage: 10Gi
      volumeName: mysql-pv
    
    ```
    
4. deployment
    
    ```yaml
    apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: mysql
      namespace: tiktok
    spec:
      replicas: 1
      selector:
        matchLabels:
          app: mysql
      template:
        metadata:
          labels:
            app: mysql
        spec:
          nodeSelector:
            kubernetes.io/hostname: node2
          containers:
            - name: mysql
              image: mysql:8.0.36
              ports:
                - containerPort: 3306
              env:
                - name: MYSQL_ROOT_PASSWORD
                  value: root
                - name: MYSQL_DATABASE
                  value: tiktok
                - name: TZ
                  value: Asia/Shanghai
              volumeMounts:
                - name: config
                  mountPath: /etc/my.cnf
                  subPath: my.cnf
                - name: init-sql
                  mountPath: /docker-entrypoint-initdb.d/1_init.sql
                  subPath: 1_init.sql
                - name: init-sql
                  mountPath: /docker-entrypoint-initdb.d/2_tiktok_backup.sql
                  subPath: 2_tiktok_backup.sql
                - name: mysql-data
                  mountPath: /var/lib/mysql
          volumes:
            - name: config
              configMap:
                name: mysql-config
            - name: init-sql
              configMap:
                name: mysql-init-sql
            - name: mysql-data
              persistentVolumeClaim:
                claimName: mysql-pvc
    
    ```
    
5. service
    
    ```yaml
    apiVersion: v1
    kind: Service
    metadata:
      name: mysql
      namespace: tiktok
    spec:
      type: NodePort
      selector:
        app: mysql
      ports:
        - port: 3306
          targetPort: 3306
          nodePort: 30306  # 30000~32767 范围内
    
    ```
    
6. 部署
    
    ```yaml
    # 1. 创建 Namespace
    kubectl apply -f k8s/mysql/namespace.yaml
    
    # 2. 创建 PV
    kubectl apply -f k8s/mysql/mysql-pv.yaml
    
    # 3. 创建 PVC（绑定刚才的 PV）
    kubectl apply -f k8s/mysql/mysql-pvc.yaml
    
    # 4. 创建 ConfigMap（从文件）
    kubectl create configmap mysql-config \
      --from-file=my.cnf=k8s/mysql/my.cnf \
      -n tiktok
    
    kubectl create configmap mysql-init-sql \
      --from-file=1_init.sql=k8s/mysql/init.sql \
      --from-file=2_tiktok_backup.sql=k8s/mysql/tiktok_backup.sql \
      -n tiktok
    
    # 5. 部署 Deployment（使用 PVC、ConfigMap、固定 node2）
    kubectl apply -f k8s/mysql/deployment.yaml
    
    # 6. 创建 Service（暴露 NodePort 外部可访问）
    kubectl apply -f k8s/mysql/service.yaml
    ```
    
7. 测试
    
    ```yaml
    kubectl get pods -n tiktok -o wide
    kubectl get svc -n tiktok
    
    kubectl get pv
    kubectl get pvc -n tiktok
    
    kubectl exec -it mysql-xxxxxxxxxx -n tiktok -- bash
    ```
    

## 3. redis

1. redis-pv.yaml
    
    ```yaml
    apiVersion: v1
    kind: PersistentVolume           # 定义的是持久卷
    metadata:
      name: redis-pv                 # PV 的名称，PVC 会通过这个名称来绑定它
    spec:
      capacity:
        storage: 1Gi                 # 存储容量，这里是 1 GiB
      accessModes:
        - ReadWriteOnce             # 只允许一个节点以读写模式挂载这个卷
      persistentVolumeReclaimPolicy: Retain  # 删除 PVC 后，保留 PV 中的数据
      hostPath:
        path: /data/redis            # 实际在 node2 上的数据路径
      nodeAffinity:                  # 限制这个卷只能被 node2 节点使用
        required:
          nodeSelectorTerms:
            - matchExpressions:
                - key: kubernetes.io/hostname  # 这个是系统自动给每个节点打的标签，值就是节点名（比如 node1, node2）
                  operator: In       # 表示“在这些值中”
                  values:
                    - node2          # 只能在 node2 节点上使用
    
    ```
    
2. redis-pvc.yaml
    
    ```yaml
    apiVersion: v1
    kind: PersistentVolumeClaim      # 定义的是“申请”一块存储资源
    metadata:
      name: redis-pvc                # PVC 的名字，在 Deployment 中引用
      namespace: tiktok
    spec:
      accessModes:
        - ReadWriteOnce              # 请求的访问模式要与 PV 保持一致
      resources:
        requests:
          storage: 1Gi               # 请求的存储大小
      volumeName: redis-pv           # 指定要绑定的 PV 名称（手动绑定）
    
    ```
    
3. deployment.yaml
    
    ```yaml
    apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: redis
      namespace: tiktok
    spec:
      replicas: 1                    # 只运行一个 Redis 实例
      selector:
        matchLabels:
          app: redis                 # 用于匹配 Pod 的标签
      template:
        metadata:
          labels:
            app: redis
        spec:
          nodeSelector:              # 指定 pod 要调度到 node2 节点
            kubernetes.io/hostname: node2
          containers:
            - name: redis
              image: redis:7.2.4     # Redis 镜像版本
              ports:
                - containerPort: 6379  # Redis 默认端口
              volumeMounts:
                - name: redis-data   # 定义挂载的数据卷
                  mountPath: /data   # Redis 会将数据保存在此目录中
          volumes:
            - name: redis-data
              persistentVolumeClaim:
                claimName: redis-pvc # 挂载上面申请的 PVC
    
    ```
    
4. service.yaml
    
    ```yaml
    apiVersion: v1
    kind: Service
    metadata:
      name: redis
      namespace: tiktok
    spec:
      type: NodePort                 # 对外暴露端口
      selector:
        app: redis                   # 匹配 Pod 的标签
      ports:
        - port: 6379                 # 集群内部访问端口
          targetPort: 6379          # 容器内部 Redis 的端口
          nodePort: 30379           # 外部通过这个端口访问 Redis
    
    ```
    
    | 项目 | 含义 |
    | --- | --- |
    | `PersistentVolume (PV)` | 管理员提供的**实际存储资源**，可以是本地目录、NFS、云磁盘等 |
    | `PersistentVolumeClaim (PVC)` | 用户向集群申请**某种大小和访问权限的存储卷** |
    | **绑定关系** | 当 PVC 和 PV 的属性（如容量、访问模式）匹配时，K8s 会自动或手动将 PVC 绑定到某个 PV |
    | **挂载到 Pod** | Pod 中通过 `volumeMounts` 使用 PVC，从而间接使用绑定的 PV（磁盘） |
5. 持久化文件
    
    ```yaml
    // node2
    mkdir -p /data/redis
    chmod 777 /data/redis
    ```
    
6. 部署
    
    ```yaml
    kubectl apply -f k8s/redis/redis-pv.yaml
    kubectl apply -f k8s/redis/redis-pvc.yaml
    kubectl apply -f k8s/redis/deployment.yaml
    kubectl apply -f k8s/redis/service.yaml
    ```
    

## 4. kafka

1. kafka
    
    ```yaml
    # pv-pvc.yaml
    apiVersion: v1
    kind: PersistentVolume
    metadata:
      name: kafka-pv
      namespace: tiktok
    spec:
      capacity:
        storage: 2Gi
      accessModes:
        - ReadWriteOnce
      persistentVolumeReclaimPolicy: Retain
      hostPath:
        path: /data/kafka
      nodeAffinity:
        required:
          nodeSelectorTerms:
            - matchExpressions:
                - key: kubernetes.io/hostname
                  operator: In
                  values:
                    - node2
    ---
    apiVersion: v1
    kind: PersistentVolumeClaim
    metadata:
      name: kafka-pvc
      namespace: tiktok
    spec:
      accessModes:
        - ReadWriteOnce
      resources:
        requests:
          storage: 2Gi
    
          
    # deployment.yaml
    [root@master kafka]# cat kafka-deployment.yaml
    apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: kafka
      namespace: tiktok
    spec:
      replicas: 1
      selector:
        matchLabels:
          app: kafka
      template:
        metadata:
          labels:
            app: kafka
        spec:
          nodeSelector:
            kubernetes.io/hostname: node2
          containers:
            - name: kafka
              image: bitnami/kafka:3.6.1
              imagePullPolicy: IfNotPresent
              ports:
                - containerPort: 19092
              env:
                - name: KAFKA_ENABLE_KRAFT
                  value: "yes"
                - name: KAFKA_CFG_NODE_ID
                  value: "0"
                - name: KAFKA_CFG_PROCESS_ROLES
                  value: "broker,controller"
                - name: KAFKA_CFG_CONTROLLER_LISTENER_NAMES
                  value: "CONTROLLER"
                - name: KAFKA_CFG_LISTENERS
                  value: "PLAINTEXT://:19092,CONTROLLER://:9093"
                - name: KAFKA_CFG_ADVERTISED_LISTENERS
                  value: "PLAINTEXT://kafka.tiktok.svc.cluster.local:19092"
                - name: KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP
                  value: "CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT"
                - name: KAFKA_CFG_INTER_BROKER_LISTENER_NAME
                  value: "PLAINTEXT"
                - name: KAFKA_CFG_CONTROLLER_QUORUM_VOTERS
                  value: "0@kafka:9093"
              volumeMounts:
                - name: kafka-data
                  mountPath: /bitnami/kafka
          volumes:
            - name: kafka-data
              persistentVolumeClaim:
                claimName: kafka-pvc
    
      
      # service.yaml
    apiVersion: v1
    kind: Service
    metadata:
      name: kafka
      namespace: tiktok
    spec:
      selector:
        app: kafka
      ports:
        - name: broker
          port: 19092
          targetPort: 19092
        - name: controller
          port: 9093
          targetPort: 9093
    
    ```
    
2. kafka-ui
    
    ```yaml
    apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: kafdrop
      namespace: tiktok
    spec:
      replicas: 1
      selector:
        matchLabels:
          app: kafdrop
      template:
        metadata:
          labels:
            app: kafdrop
        spec:
          containers:
            - name: kafdrop
              image: obsidiandynamics/kafdrop:latest
              imagePullPolicy: IfNotPresent
              ports:
                - containerPort: 9000
              env:
                - name: KAFKA_BROKERCONNECT
                  value: "kafka:19092"  # 或者用你的实际 ClusterIP / Service 名字和端口
    ---
    apiVersion: v1
    kind: Service
    metadata:
      name: kafdrop
      namespace: tiktok
    spec:
      selector:
        app: kafdrop
      ports:
        - protocol: TCP
          port: 9000
          targetPort: 9000
      type: NodePort
    
    ```
    
3. canal
    
    ```yaml
    apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: canal
      namespace: tiktok
      labels:
        app: canal
    spec:
      replicas: 1  # 只部署一个实例
      selector:
        matchLabels:
          app: canal
      template:
        metadata:
          labels:
            app: canal
        spec:
          initContainers:
          - name: wait-for-mysql
            image: busybox:1.35
            command: ['sh', '-c', 'until nc -z mysql 3306; do echo waiting for mysql; sleep 2; done']
          - name: wait-for-kafka
            image: busybox:1.35
            command: ['sh', '-c', 'until nc -z kafka 19092; do echo waiting for kafka; sleep 2; done']
          nodeSelector:
            kubernetes.io/hostname: node2  # 保证 Pod 被调度到 node2
          containers:
            - name: canal
              image: canal/canal-server:latest
              imagePullPolicy: IfNotPresent
              ports:
                - containerPort: 11111  # 默认 Canal Server 的 TCP 端口
              env:
                - name: canal.instance.master.address
                  value: "mysql:3306"  # 与你 MySQL 的 service 名一致
                - name: canal.instance.dbUsername
                  value: "ysh"
                - name: canal.instance.dbPassword
                  value: "ysh"
                - name: canal.auto.scan
                  value: "true"
              volumeMounts:
                - name: canal-conf
                  mountPath: /home/admin/canal-server/conf
    
          volumes:
            - name: canal-conf
              hostPath:
                path: /data/canal/canal-conf  # node2 上的配置目录
                type: Directory
    
    apiVersion: v1
    kind: Service
    metadata:
      name: canal
      namespace: tiktok
    spec:
      selector:
        app: canal
      ports:
        - name: tcp
          port: 11111
          targetPort: 11111
          protocol: TCP
      type: ClusterIP  # 或 NodePort 如果你想从集群外访问
    
    # node2 
    将canal配置文件上传，删除meta
    
    kubectl apply -f ../zoo1/zoo1.yaml
    mkdir kafak
    chmod 777 kafka
    kubectl apply -f kafka-pv-pvc.yaml
    kubectl apply -f kafka-deployment.yaml
    kubectl apply -f kafka-service.yaml
    kubectl apply -f kafka-ui.yaml
    kubectl apply -f canal-deployment.yaml
    kubectl apply -f canal-service.yaml
    ```
    

要重新配置就全删了

```yaml
kubectl get pod -n tiktok
kubectl describe pod -n tiktok

kubectl delete deployment zoo1 -n tiktok
kubectl delete deployment kafka -n tiktok
kubectl delete deployment kafka-ui -n tiktok
kubectl delete deployment canal -n tiktok

kubectl delete service zoo1 -n tiktok
kubectl delete service kafka -n tiktok
kubectl delete service kafka-ui -n tiktok
kubectl delete service canal -n tiktok

kubectl delete pvc kafka-pvc -n tiktok
kubectl delete pv kafka-pv

// 加镜像

vim /etc/docker/daemon.json
sudo systemctl daemon-reload
sudo systemctl restart docker

kubectl delete deployment elasticsearch -n tiktok
kubectl delete deployment kibana -n tiktok
kubectl delete service elasticsearch  -n tiktok
kubectl delete service kibana -n tiktok
kubectl delete pvc es-pvc -n tiktok
kubectl delete pv es-pv
```

## 5. es

1. es
    
    ```yaml
    apiVersion: v1
    kind: PersistentVolume
    metadata:
      name: es-pv
      namespace: tiktok
    spec:
      capacity:
        storage: 2Gi
      accessModes:
        - ReadWriteOnce
      persistentVolumeReclaimPolicy: Retain
      hostPath:
        path: /data/es
      nodeAffinity:
        required:
          nodeSelectorTerms:
            - matchExpressions:
                - key: kubernetes.io/hostname
                  operator: In
                  values:
                    - node2
    ---
    apiVersion: v1
    kind: PersistentVolumeClaim
    metadata:
      name: es-pvc
      namespace: tiktok
    spec:
      accessModes:
        - ReadWriteOnce
      resources:
        requests:
          storage: 2Gi
    
    apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: elasticsearch
      namespace: tiktok
    spec:
      replicas: 1
      selector:
        matchLabels:
          app: elasticsearch
      template:
        metadata:
          labels:
            app: elasticsearch
        spec:
          nodeSelector:
            kubernetes.io/hostname: node2
          initContainers:
          - name: wait-for-kafka
            image: busybox:1.35
            command: ['sh', '-c', 'until nc -z kafka 19092; do echo waiting for kafka; sleep 2; done']
          containers:
          - name: elasticsearch
            image: docker.elastic.co/elasticsearch/elasticsearch:7.17.10
            ports:
            - containerPort: 9200
            - containerPort: 9300
            env:
            - name: discovery.type
              value: single-node
            - name: ES_JAVA_OPTS
              value: "-Xms256m -Xmx256m"
            - name: xpack.security.enabled
              value: "false"
            resources:
              requests:
                memory: "512Mi"               # 请求512MB内存
                cpu: "250m"                  # 请求0.25个CPU
              limits:
                memory: "768Mi"              # 限制最大768MB内存
                cpu: "500m"                  # 限制最大0.5个CPU
            volumeMounts:
              - name: es-data
                mountPath: /usr/share/elasticsearch/data
          volumes:
          - name: es-data
            persistentVolumeClaim:
              claimName: es-pvc
    
    apiVersion: v1
    kind: Service
    metadata:
      name: elasticsearch
      namespace: tiktok
    spec:
      selector:
        app: elasticsearch
      ports:
        - name: http
          port: 9200
          targetPort: 9200
        - name: transport
          port: 9300
          targetPort: 9300
      type: ClusterIP
    
    ```
    
2. ui
    
    ```yaml
    kind: Deployment
    metadata:
      name: kibana
      namespace: tiktok
    spec:
      replicas: 1
      selector:
        matchLabels:
          app: kibana
      template:
        metadata:
          labels:
            app: kibana
        spec:
          containers:
            - name: kibana
              image: docker.elastic.co/kibana/kibana:7.17.10
              ports:
                - containerPort: 5601
              env:
                - name: ELASTICSEARCH_HOSTS
                  value: "http://elasticsearch.tiktok.svc.cluster.local:9200"
                - name: XPACK_SECURITY_ENABLED
                  value: "false"
    ---
    apiVersion: v1
    kind: Service
    metadata:
      name: kibana
      namespace: tiktok
    spec:
      selector:
        app: kibana
      ports:
        - port: 5601
          targetPort: 5601
      type: NodePort
    ```
    

# 虚拟机

1. 虚拟机连接不上的问题
    
    ```yaml
    https://cloud.tencent.com/developer/article/2110372
    ```
    
2. k8s部署框架问题
    
    ```yaml
    uname -m
    x86_64
    
    在编译的时候要转为amd的 // dockerfile
    RUN CGO_ENABLED=0 GOOS=linux GOARCH=amd64 go build -o bin/job-service ./cmd/job-service
    ```